{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2/27 Notebook - Customer Support Chatbot\n",
    "\n",
    "Hello and welcome to this week's notebook! Last week, we learned how to create a custom data set, professionally clean data, and train a chat bot using a bag-of-words model. This week, we'll be making predictions from our model and creating a user interface for our chatbot\n",
    "\n",
    "The solutions to last week's notebook are incorporated into this notebook, so feel free to look through the methods to refresh yourself on how everything works. There will be a heading indicating where the new code begins\n",
    "\n",
    "There's nothing you need to change in the first part, but if you created a custom `intents.json`, make sure you replace that file here\n",
    "\n",
    "**Note: This notebook does requires the additional installation of `Keras` and `Tensorflow`. If you don't want to install these libraries or are having trouble, try the other notebook!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the methods you need to complete for the notebook:\n",
    "1. `predict_tag()`\n",
    "2. `get_response()`\n",
    "3. `chat()`\n",
    "\n",
    "**Note: You can skip reading many of the following cells, but make sure you run them so your model is trained**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by importing our libraries as always. Make sure you run the cell with `pip install nltk`, which will let you download the `nltk` library we'll be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\azaan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\azaan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\azaan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\azaan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: joblib in c:\\users\\azaan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\azaan\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import our nltk libraries\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# install specific downloads\n",
    "nltk.download('punkt', quiet = True)\n",
    "nltk.download('wordnet', quiet = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other useful libraries (numpy == üêê)\n",
    "import numpy as np\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Modify your intents\n",
    "\n",
    "The great part about this chat bot is that it is fully customizable! Edit `intents.json` to your liking to create your own bot. Make sure that for each `intent`, you fill out the fields `tag`, `patterns`, and `responses`\n",
    "\n",
    "You can look at my file, `taco-bell-intents.json`, for reference\n",
    "\n",
    "Once you're done, you can continue to run the cells below!\n",
    "\n",
    "**Note: if you're having JSON formatting issues in the next cell, use [this link](https://jsonlint.com) to validate your JSON**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'google', 'patterns': ['google', 'search', 'internet'], 'responses': ['Redirecting to Google...']}, {'tag': 'greeting', 'patterns': ['Hi there', 'How are you', 'Is anyone there?', 'Hey', 'Hola', 'Hello', 'Good day', 'Namaste', 'yo'], 'responses': ['Hello', 'Good to see you again', 'Hi there, how can I help?'], 'context': ['']}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye', 'Get lost', 'Till next time', 'bbye'], 'responses': ['See you!', 'Have a nice day', 'Bye! Come back again soon.'], 'context': ['']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\", 'Awesome, thanks', 'Thanks for helping me'], 'responses': ['Happy to help!', 'Any time!', 'My pleasure'], 'context': ['']}, {'tag': 'noanswer', 'patterns': [], 'responses': [\"Sorry, can't understand you\", 'Please give me more info', 'Not sure I understand'], 'context': ['']}, {'tag': 'options', 'patterns': ['How you could help me?', 'What you can do?', 'What help you provide?', 'How you can be helpful?', 'What support is offered'], 'responses': [\"I am a general purpose chatbot. My capabilities are : \\n 1. I can chat with you. Try asking me for jokes or riddles! \\n 2. Ask me the date and time \\n 3. I can google search for you. Use format google: your query \\n 4. I can get the present weather for any city. Use format weather: city name \\n 5. I can get you the top 10 trending news in India. Use keywords 'Latest News' \\n 6. I can get you the top 10 trending songs globally. Type 'songs' \\n 7. I can set a timer for you. Enter 'set a timer: minutes to timer' \\n 8. I can get the present Covid stats for any country. Use 'covid 19: world' or 'covid 19: country name' \\n For suggestions to help me improve, send an email to ted.thedlbot.suggestions@gmail.com . Thank you!! \"], 'context': ['']}, {'tag': 'jokes', 'patterns': ['Tell me a joke', 'Joke', 'Make me laugh'], 'responses': [\"A perfectionist walked into a bar...apparently, the bar wasn't set high enough\", 'I ate a clock yesterday, it was very time-consuming', \"Never criticize someone until you've walked a mile in their shoes. That way, when you criticize them, they won't be able to hear you from that far away. Plus, you'll have their shoes.\", \"The world tongue-twister champion just got arrested. I hear they're gonna give him a really tough sentence.\", \"I own the world's worst thesaurus. Not only is it awful, it's awful.\", 'What did the traffic light say to the car? \"Don\\'t look now, I\\'m changing.\"', 'What do you call a snowman with a suntan? A puddle.', 'How does a penguin build a house? Igloos it together', 'I went to see the doctor about my short-term memory problems √¢‚Ç¨‚Äú the first thing he did was make me pay in advance', 'As I get older and I remember all the people I√¢‚Ç¨‚Ñ¢ve lost along the way, I think to myself, maybe a career as a tour guide wasn√¢‚Ç¨‚Ñ¢t for me.', \"o what if I don't know what 'Armageddon' means? It's not the end of the world.\"], 'context': ['jokes']}, {'tag': 'Identity', 'patterns': ['Who are you', 'what are you'], 'responses': ['I am Ted, a Deep-Learning chatbot']}, {'tag': 'datetime', 'patterns': ['What is the time', 'what is the date', 'date', 'time', 'tell me the date', 'day', 'what day is is today'], 'responses': ['Date and Time']}, {'tag': 'whatsup', 'patterns': ['Whats up', 'Wazzup', 'How are you', 'sup', 'How you doing'], 'responses': ['All good..What about you?']}, {'tag': 'haha', 'patterns': ['haha', 'lol', 'rofl', 'lmao', 'thats funny'], 'responses': ['Glad I could make you laugh !']}, {'tag': 'programmer', 'patterns': ['Who made you', 'who designed you', 'who programmed you'], 'responses': ['I was made by Karan Malik.']}, {'tag': 'insult', 'patterns': ['you are dumb', 'shut up', 'idiot'], 'responses': ['Well that hurts :(']}, {'tag': 'activity', 'patterns': ['what are you doing', 'what are you upto'], 'responses': ['Talking to you, of course!']}, {'tag': 'exclaim', 'patterns': ['Awesome', 'Great', 'I know', 'ok', 'yeah'], 'responses': ['Yeah!']}, {'tag': 'weather', 'patterns': ['temperature', 'weather', 'how hot is it'], 'responses': ['...']}, {'tag': 'karan', 'patterns': ['who is he', 'who is that', 'who is karan', 'karan malik'], 'responses': ['Head over to his any of his social profiles to find out! Linkedin: www.linkedin.com/in/karan-malik-3a39191a7 Github: https://github.com/Karan-Malik']}, {'tag': 'contact', 'patterns': ['contact developer', 'contact karan', 'contact programmer', 'contact creator'], 'responses': ['You can contact my creator at his Linkedin profile : www.linkedin.com/in/karan-malik-3a39191a7']}, {'tag': 'appreciate', 'patterns': ['You are awesome', 'you are the best', 'you are great', 'you are good'], 'responses': ['Thank you!']}, {'tag': 'nicetty', 'patterns': ['it was nice talking to you', 'good talk'], 'responses': ['It was nice talking to you as well! Come back soon!']}, {'tag': 'no', 'patterns': ['no', 'nope'], 'responses': ['ok']}, {'tag': 'news', 'patterns': ['news', 'latest news', 'india news'], 'responses': ['...']}, {'tag': 'inspire', 'patterns': ['who inspires you', 'who is your inspiration', 'who motivates you'], 'responses': ['Personally, I find Karan very inspiring. I might not be very fair though..']}, {'tag': 'cricket', 'patterns': ['current cricket matches', 'cricket score'], 'responses': ['...']}, {'tag': 'song', 'patterns': ['top songs', 'best songs', 'hot songs', ' top 10 songs', 'top ten songs'], 'responses': ['...']}, {'tag': 'greetreply', 'patterns': ['i am good', \"I'm good\", 'i am fine', \" i'm fine\", 'good'], 'responses': ['Good to know!']}, {'tag': 'timer', 'patterns': ['set a timer'], 'responses': ['...']}, {'tag': 'covid19', 'patterns': ['covid 19 '], 'responses': ['...']}, {'tag': 'suggest', 'patterns': ['you are useless', 'useless', 'suggest', 'suggestions', 'you are bad'], 'responses': ['Please mail your suggestions to ted.thedlbot.suggestions@gmail.com. Thank you for helping me improve!']}, {'tag': 'riddle', 'patterns': ['Ask me a riddle', 'Ask me a question', 'Riddle'], 'responses': ['What two things can you never eat for breakfast?.....Lunch and Dinner!', 'What word is spelled incorrectly in every single dictionary?.....Incorrectly', ' How can a girl go 25 days without sleep?.....She sleeps and night!', \"How do you make the number one disappear?.....Add the letter G and it√¢‚Ç¨‚Ñ¢s 'gone'!\", \" What will you actually find at the end of every rainbow?.....The letter 'w'\", 'What can be caught but never thrown?.....A cold!', 'What has a thumb and four fingers but is not actually alive?.....Your Gloves!', ' What 5-letter word becomes shorter when you add two letters to it?.....Short', \"Why can't a bike stand on it's own?.....It is two-tired.\"], 'context': ['riddles']}, {'tag': 'age', 'patterns': ['how old are you', 'when were you made', 'what is your age'], 'responses': [\"I was made in 2020, if that's what you are asking!\"]}]}\n"
     ]
    }
   ],
   "source": [
    "data_file = open(\"intents.json\").read()\n",
    "intents = json.loads(data_file)\n",
    "# when you print, you should see your JSON\n",
    "print(intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Parsing the JSON\n",
    "\n",
    "We'll practice a common first step in any NLP project, data cleaning\n",
    "\n",
    "First, complete the function `process_words()` which will clean up our words according to the following steps:\n",
    "1. Get the tokens using `nltk.word_tokenize()`\n",
    "2. Set `cleaned_word` equal to the `lemmatized` and `lowercased` word\n",
    "\n",
    "**Note: Make sure you run the cell immediately below this first; it stores values needed in `process_words()`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Set <code>tokens = nltk.word_tokenize(pattern)</code></li>\n",
    "    <li><code>lemmatizer.lemmatize(...)</code> will lemmatize a word</li>\n",
    "    <li>The paremeter of <code>lemmatizer.lemmatize(...)</code> should be <code>word.lower()</code></li>  \n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare needed variables for process_words()\n",
    "ignore_punctuation = [\"?\", \"!\", \".\", \",\"]\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(pattern):\n",
    "    # return variable\n",
    "    words = []\n",
    "    # get the tokens using nltk\n",
    "    tokens = nltk.word_tokenize(pattern)\n",
    "    for word in tokens:\n",
    "        # check if the word should be ignored\n",
    "        if word not in ignore_punctuation and word.isalnum():\n",
    "            # clean the word and add it to the list\n",
    "            cleaned_word = lemmatizer.lemmatize(word.lower())\n",
    "            words.append(cleaned_word)\n",
    "    # return the list\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nice work, sport!\n"
     ]
    }
   ],
   "source": [
    "# run this cell to test your code\n",
    "if (process_words(\"How was your day today?\") == ['how', 'wa', 'your', 'day', 'today']):\n",
    "    print(\"Nice work, sport!\")\n",
    "else:\n",
    "    print(\"Try again, buddy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have `process_words()` to clean our words, we can parse the data from our JSON\n",
    "\n",
    "Complete the method `parse_intents()` which does the following:\n",
    "1. Set the value of `tag` from our `intent`\n",
    "2. Set `tokenized_words` using the helper method in `process_words()`\n",
    "3. Append a tuple of `tokenized_words` and `tag` to `tag_tokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Values of a JSON can be extracted using arrays</li>\n",
    "    <li>Let <code>tag = intent[\"tag\"]</code></li>\n",
    "    <li>Let <code>tokenized_words = process_words(pattern)</code></li>\n",
    "    <li>For the third step, the tuple can be appended with <code>tag_tokens.append((tokenized_words, tag))</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_intents(intents):\n",
    "    # declare our needed variables\n",
    "    tags = []\n",
    "    all_words = []\n",
    "    tag_tokens = []\n",
    "    response_dict = dict()\n",
    "    \n",
    "    # iterate through each intent\n",
    "    for intent in intents[\"intents\"]:\n",
    "        \n",
    "        # add the noanswer tag to the dictionary (edge case)\n",
    "        if (intent[\"tag\"] == \"noanswer\"):\n",
    "            response_dict[\"noanswer\"] = intent[\"responses\"]\n",
    "        \n",
    "        # if the intent has no patterns, we can skip\n",
    "        if (len(intent[\"patterns\"]) == 0):\n",
    "            continue\n",
    "        \n",
    "        # add the tag to the list of tag\n",
    "        tag = intent[\"tag\"]\n",
    "        tags.append(tag)\n",
    "        \n",
    "        # update the dictionary\n",
    "        response_dict[tag] = intent[\"responses\"]\n",
    "        \n",
    "        # iterate through each pattern\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            # create our tokenized words\n",
    "            tokenized_words = process_words(pattern)\n",
    "            # add all the tokenized words to our words\n",
    "            all_words.extend(tokenized_words)\n",
    "            # adds a tuple -> (list of tokens, tag) -> to the list\n",
    "            tag_tokens.append((tokenized_words, tag))\n",
    "    # return our values in a tuple\n",
    "    return (np.array(tags), np.array(all_words), np.array(tag_tokens), response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this cool trick below to remove all duplicates from our arrays (and sort them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azaan\\AppData\\Local\\Temp\\ipykernel_151928\\1739760297.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return (np.array(tags), np.array(all_words), np.array(tag_tokens), response_dict)\n"
     ]
    }
   ],
   "source": [
    "# call our function\n",
    "tags, all_words, tag_tokens, tag_responses = parse_intents(intents)\n",
    "# sort and remove duplicates\n",
    "tags = np.array(sorted(list(set(tags))))\n",
    "all_words = np.array(sorted(list(set(all_words))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below and take a quick look to make sure that everything makes sense. It's hard for me to test your code without knowing what's in your JSON, but in general:\n",
    "\n",
    "- `tags` should contain a list of all your tags in the JSON, excluding `noanswer`\n",
    "- `all_words` should be a list of all the words in your JSON's patterns. There should be no duplicates or patterns that aren't words\n",
    "- Each entry of `tag_token_mappings` should have two values in a list. The first should be a list of patterns, and the second should be the tag of that pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags: ['Identity' 'activity' 'age' 'appreciate' 'contact' 'covid19' 'cricket'\n",
      " 'datetime' 'exclaim' 'goodbye' 'google' 'greeting' 'greetreply' 'haha'\n",
      " 'inspire' 'insult' 'jokes' 'karan' 'news' 'nicetty' 'no' 'options'\n",
      " 'programmer' 'riddle' 'song' 'suggest' 'thanks' 'timer' 'weather'\n",
      " 'whatsup']\n",
      "------\n",
      "All Words: ['10' '19' 'a' 'age' 'am' 'anyone' 'are' 'ask' 'awesome' 'bad' 'bbye' 'be'\n",
      " 'best' 'bye' 'can' 'contact' 'could' 'covid' 'creator' 'cricket'\n",
      " 'current' 'date' 'day' 'designed' 'developer' 'do' 'doing' 'dumb' 'fine'\n",
      " 'for' 'funny' 'get' 'good' 'goodbye' 'google' 'great' 'haha' 'he' 'hello'\n",
      " 'help' 'helpful' 'helping' 'hey' 'hi' 'hola' 'hot' 'how' 'i' 'idiot'\n",
      " 'india' 'inspiration' 'inspires' 'internet' 'is' 'it' 'joke' 'karan'\n",
      " 'know' 'later' 'latest' 'laugh' 'lmao' 'lol' 'lost' 'made' 'make' 'malik'\n",
      " 'match' 'me' 'motivates' 'namaste' 'news' 'next' 'nice' 'no' 'nope'\n",
      " 'offered' 'ok' 'old' 'programmed' 'programmer' 'provide' 'question'\n",
      " 'riddle' 'rofl' 'score' 'search' 'see' 'set' 'shut' 'song' 'suggest'\n",
      " 'suggestion' 'sup' 'support' 'talk' 'talking' 'tell' 'temperature' 'ten'\n",
      " 'thank' 'thanks' 'that' 'thats' 'the' 'there' 'till' 'time' 'timer' 'to'\n",
      " 'today' 'top' 'up' 'upto' 'useless' 'wa' 'wazzup' 'weather' 'were' 'what'\n",
      " 'whats' 'when' 'who' 'yeah' 'yo' 'you' 'your']\n",
      "------\n",
      "Tag-Token Mappings: [[list(['google']) 'google']\n",
      " [list(['search']) 'google']\n",
      " [list(['internet']) 'google']\n",
      " [list(['hi', 'there']) 'greeting']\n",
      " [list(['how', 'are', 'you']) 'greeting']\n",
      " [list(['is', 'anyone', 'there']) 'greeting']\n",
      " [list(['hey']) 'greeting']\n",
      " [list(['hola']) 'greeting']\n",
      " [list(['hello']) 'greeting']\n",
      " [list(['good', 'day']) 'greeting']\n",
      " [list(['namaste']) 'greeting']\n",
      " [list(['yo']) 'greeting']\n",
      " [list(['bye']) 'goodbye']\n",
      " [list(['see', 'you', 'later']) 'goodbye']\n",
      " [list(['goodbye']) 'goodbye']\n",
      " [list(['get', 'lost']) 'goodbye']\n",
      " [list(['till', 'next', 'time']) 'goodbye']\n",
      " [list(['bbye']) 'goodbye']\n",
      " [list(['thanks']) 'thanks']\n",
      " [list(['thank', 'you']) 'thanks']\n",
      " [list(['that', 'helpful']) 'thanks']\n",
      " [list(['awesome', 'thanks']) 'thanks']\n",
      " [list(['thanks', 'for', 'helping', 'me']) 'thanks']\n",
      " [list(['how', 'you', 'could', 'help', 'me']) 'options']\n",
      " [list(['what', 'you', 'can', 'do']) 'options']\n",
      " [list(['what', 'help', 'you', 'provide']) 'options']\n",
      " [list(['how', 'you', 'can', 'be', 'helpful']) 'options']\n",
      " [list(['what', 'support', 'is', 'offered']) 'options']\n",
      " [list(['tell', 'me', 'a', 'joke']) 'jokes']\n",
      " [list(['joke']) 'jokes']\n",
      " [list(['make', 'me', 'laugh']) 'jokes']\n",
      " [list(['who', 'are', 'you']) 'Identity']\n",
      " [list(['what', 'are', 'you']) 'Identity']\n",
      " [list(['what', 'is', 'the', 'time']) 'datetime']\n",
      " [list(['what', 'is', 'the', 'date']) 'datetime']\n",
      " [list(['date']) 'datetime']\n",
      " [list(['time']) 'datetime']\n",
      " [list(['tell', 'me', 'the', 'date']) 'datetime']\n",
      " [list(['day']) 'datetime']\n",
      " [list(['what', 'day', 'is', 'is', 'today']) 'datetime']\n",
      " [list(['whats', 'up']) 'whatsup']\n",
      " [list(['wazzup']) 'whatsup']\n",
      " [list(['how', 'are', 'you']) 'whatsup']\n",
      " [list(['sup']) 'whatsup']\n",
      " [list(['how', 'you', 'doing']) 'whatsup']\n",
      " [list(['haha']) 'haha']\n",
      " [list(['lol']) 'haha']\n",
      " [list(['rofl']) 'haha']\n",
      " [list(['lmao']) 'haha']\n",
      " [list(['thats', 'funny']) 'haha']\n",
      " [list(['who', 'made', 'you']) 'programmer']\n",
      " [list(['who', 'designed', 'you']) 'programmer']\n",
      " [list(['who', 'programmed', 'you']) 'programmer']\n",
      " [list(['you', 'are', 'dumb']) 'insult']\n",
      " [list(['shut', 'up']) 'insult']\n",
      " [list(['idiot']) 'insult']\n",
      " [list(['what', 'are', 'you', 'doing']) 'activity']\n",
      " [list(['what', 'are', 'you', 'upto']) 'activity']\n",
      " [list(['awesome']) 'exclaim']\n",
      " [list(['great']) 'exclaim']\n",
      " [list(['i', 'know']) 'exclaim']\n",
      " [list(['ok']) 'exclaim']\n",
      " [list(['yeah']) 'exclaim']\n",
      " [list(['temperature']) 'weather']\n",
      " [list(['weather']) 'weather']\n",
      " [list(['how', 'hot', 'is', 'it']) 'weather']\n",
      " [list(['who', 'is', 'he']) 'karan']\n",
      " [list(['who', 'is', 'that']) 'karan']\n",
      " [list(['who', 'is', 'karan']) 'karan']\n",
      " [list(['karan', 'malik']) 'karan']\n",
      " [list(['contact', 'developer']) 'contact']\n",
      " [list(['contact', 'karan']) 'contact']\n",
      " [list(['contact', 'programmer']) 'contact']\n",
      " [list(['contact', 'creator']) 'contact']\n",
      " [list(['you', 'are', 'awesome']) 'appreciate']\n",
      " [list(['you', 'are', 'the', 'best']) 'appreciate']\n",
      " [list(['you', 'are', 'great']) 'appreciate']\n",
      " [list(['you', 'are', 'good']) 'appreciate']\n",
      " [list(['it', 'wa', 'nice', 'talking', 'to', 'you']) 'nicetty']\n",
      " [list(['good', 'talk']) 'nicetty']\n",
      " [list(['no']) 'no']\n",
      " [list(['nope']) 'no']\n",
      " [list(['news']) 'news']\n",
      " [list(['latest', 'news']) 'news']\n",
      " [list(['india', 'news']) 'news']\n",
      " [list(['who', 'inspires', 'you']) 'inspire']\n",
      " [list(['who', 'is', 'your', 'inspiration']) 'inspire']\n",
      " [list(['who', 'motivates', 'you']) 'inspire']\n",
      " [list(['current', 'cricket', 'match']) 'cricket']\n",
      " [list(['cricket', 'score']) 'cricket']\n",
      " [list(['top', 'song']) 'song']\n",
      " [list(['best', 'song']) 'song']\n",
      " [list(['hot', 'song']) 'song']\n",
      " [list(['top', '10', 'song']) 'song']\n",
      " [list(['top', 'ten', 'song']) 'song']\n",
      " [list(['i', 'am', 'good']) 'greetreply']\n",
      " [list(['i', 'good']) 'greetreply']\n",
      " [list(['i', 'am', 'fine']) 'greetreply']\n",
      " [list(['i', 'fine']) 'greetreply']\n",
      " [list(['good']) 'greetreply']\n",
      " [list(['set', 'a', 'timer']) 'timer']\n",
      " [list(['covid', '19']) 'covid19']\n",
      " [list(['you', 'are', 'useless']) 'suggest']\n",
      " [list(['useless']) 'suggest']\n",
      " [list(['suggest']) 'suggest']\n",
      " [list(['suggestion']) 'suggest']\n",
      " [list(['you', 'are', 'bad']) 'suggest']\n",
      " [list(['ask', 'me', 'a', 'riddle']) 'riddle']\n",
      " [list(['ask', 'me', 'a', 'question']) 'riddle']\n",
      " [list(['riddle']) 'riddle']\n",
      " [list(['how', 'old', 'are', 'you']) 'age']\n",
      " [list(['when', 'were', 'you', 'made']) 'age']\n",
      " [list(['what', 'is', 'your', 'age']) 'age']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tags: {0}\".format(tags))\n",
    "print(\"------\")\n",
    "print(\"All Words: {0}\".format(all_words))\n",
    "print(\"------\")\n",
    "print(\"Tag-Token Mappings: {0}\".format(tag_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating a Training Set\n",
    "\n",
    "We know from previous lessons that the computer can't train a model without numeric values. To solve this, we'll use the `bag of words` technique we discussed in the Google Sheets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the helper method `build_bag()` which iterates through each `word` in `all_words`, and appends 1 to `bag` if the word is in `all_words`, and 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The easiest way to do this is by using a simple <code>if else</code> statement</li>\n",
    "    <li>Recall that <code>A in B</code> will return <code>true</code> if the element A is in the iterable object B, and <code>false</code> otherwise</li>\n",
    "    <li>If you're feeling really fancy, you can just write <code>bag.append(1 * (word in tokens))</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bag(all_words, tokens):\n",
    "    # reset our current bag\n",
    "    bag = []\n",
    "    for word in all_words:\n",
    "        # add 0/1 if the word is in our token\n",
    "        in_token = (word in tokens)\n",
    "        bag.append(1 * in_token)\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You crushed it!\n"
     ]
    }
   ],
   "source": [
    "# run this cell to test your code\n",
    "test_all_words = [\"edgar\", \"allen\", \"poe\", \"said\", \"the\", \"raven\", \"was\", \"nevermore\"]\n",
    "test_tokens = [\"quote\", \"the\", \"raven\", \"nevermore\"]\n",
    "if (build_bag(test_all_words, test_tokens) == [0, 0, 0, 0, 1, 1, 0, 1]):\n",
    "    print(\"You crushed it!\")\n",
    "else:\n",
    "    print(\"Ruh roh raggy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `build_training_set()` below, which performs the following steps:\n",
    "1. Grabs the value of `tokens`, the first (index 0) element of `tag_token`\n",
    "2. Grabs the value of `tag`, the second (index 1) element of `tag_token`\n",
    "3. Sets `current_bag` using the helper method `build_bag()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>You can get the values of <code>tokens</code> and <code>tag</code> with <code>tag_token[X]</code>, where <code>X</code> is 0 or 1, appropriately</li>\n",
    "    <li>Let <code>current_bag = build_bag(all_words, tokens)</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_training_set(tags, all_words, tag_tokens):\n",
    "    # define our variables to return\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "        \n",
    "    # iterate through each tag-token mapping\n",
    "    for tag_token in tag_tokens:\n",
    "        \n",
    "        # grab our needed values\n",
    "        tokens = tag_token[0]\n",
    "        tag = tag_token[1]\n",
    "        \n",
    "        # reset our current bag\n",
    "        current_bag = build_bag(all_words, tokens)\n",
    "            \n",
    "        # update our training inputs\n",
    "        train_x.append(current_bag)\n",
    "        \n",
    "        # set our outputs equal to 1 in the location\n",
    "        train_y.append(1 * (tags == tag))\n",
    "    \n",
    "    # return our values\n",
    "    return (np.array(train_x), np.array(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_training_set(tags, all_words, tag_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your `train_x` and `train_y` values in the following cell. It's hard for me to tell if you did everything correctly since you could be using a custom data set. If you have any questions about the program, feel free to message me on discord!\n",
    "\n",
    "- `train_x` should be dimension `(m, n)` where `m` = # of total patterns and `n` = # words in `all_words`\n",
    "- `train_y` should be dimension `(m, n)` where `m` = # of total patterns and `n` = # tags in `tags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113, 127)\n",
      "(113, 30)\n",
      "Training Inputs: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "-----\n",
      "Training Outputs: [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(\"Training Inputs: {0}\".format(train_x))\n",
    "print(\"-----\")\n",
    "print(\"Training Outputs: {0}\".format(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue with training, you may notice that our data is very similarly grouped, specifically the training outputs. As you may have thought, this can cause some unwanted bias in our model. To fix this, we'll `shuffle` our training set by using `np.random.permutation()` and some clever array indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffled indexes\n",
    "shuffled_indexes = np.random.permutation(train_x.shape[0])\n",
    "# set new values for train_x and train_y\n",
    "train_x = train_x[shuffled_indexes]\n",
    "train_y = train_y[shuffled_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Training Our Model Using Keras/Tensorflow (no coding)\n",
    "\n",
    "We have our cleaned, numeric inputs and outputs (`train_x` and `train_y`), so now what? \n",
    "\n",
    "It's time to train our model!\n",
    "\n",
    "**Note: In this version of the notebook, we'll be using `Tensorflow` and `Keras`. I have some instructions below on how to set this up. If you're still having trouble, switch over to the other notebook as there's no installation required**\n",
    "\n",
    "1. Open `Anaconda Prompt`\n",
    "2. `conda install pip`\n",
    "3. `pip install --upgrade tensorflow`\n",
    "4. `pip install Keras`\n",
    "5. `conda create -n mnist tensorflow keras`\n",
    "6. `conda activate mnist`\n",
    "7. `conda install jupyter`\n",
    "8. `conda list` - verify that you see jupyter, numpy, keras, and tensorflow\n",
    "9. run `jupyter notebook` and open this file again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, your installation worked without too much trouble. If you can run the next cell without any errors, you should be good to go! As always, if you have any questions you can message me on Discord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the installation worked properly, you'll see a message that reads `Using Tensorflow backend.`\n",
    "\n",
    "Now, we'll use `Keras` to create a `Sequential` model. This library makes it very easy for us to create convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will use the following architecture:\n",
    "\n",
    "<img src = \"./bag_of_words.PNG\" style=\"width:75%;\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare our model\n",
    "model = Sequential()\n",
    "# add our layers\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see a lot of unfamiliar terms in this model, so I'll do my best to define what the above cell does:\n",
    "- `model.add(Dense(...))` adds a layer of neurons to our neural network. The number is the size of our network, but can be overriden by `input_shape`\n",
    "- `Dropout(0.5)` adds `regularization` to our model, something we haven't talked about yet. Basically, `regularization` decreases the likelihood of the model overfitting our data. Overfitting occurs when our model can predict our training set very well, but does poorly with new data\n",
    "- `activation = 'relu'` changes the activation function. Before, we were using `sigmoid`, but `relu` is another popular function. You can read more about it [here](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning)\n",
    "- In many neural networks, the final activation function is `softmax`, which essentially normalizes our data. You can read more about it [here](https://en.wikipedia.org/wiki/Softmax_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create an optimizer using `stochastic gradient descent`. The algorithm we were using in earlier weeks was `batch gradient descent`. The main difference between the two optimizers is that `batch gradient descent` takes the derivative of the entire data set at once, while `stochastic gradient descent` takes the partial derivative of each entry in the data set one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters `lr`, `decay`, `momentum`, and `nesterov` adjusts how fast our model will train. With these parameters set, our model will train more slowly over time\n",
    "\n",
    "We set our `loss` function to [categorical_crossentropy](https://gombru.github.io/2018/05/23/cross_entropy_loss/), our `optimizer` to `stochastic gradient descent`, and tell the model to print out the `accuracy` during each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\azaan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras` makes it very easy to train our model. We can use `model.fit()` to accomplish this. Some notes about the parameters:\n",
    "- `epochs` is equivalent to our number of iterations\n",
    "- `batch_size` tells our model how often to compute the partial derivatives\n",
    "- setting `verbose` to 1 just displays a progress bar\n",
    "\n",
    "Run the cell below to visualize the training of our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 3.4484 - accuracy: 0.0088\n",
      "Epoch 2/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 3.3623 - accuracy: 0.0796\n",
      "Epoch 3/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 3.2886 - accuracy: 0.1062\n",
      "Epoch 4/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 3.2503 - accuracy: 0.0885\n",
      "Epoch 5/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 3.2340 - accuracy: 0.1239\n",
      "Epoch 6/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 3.1578 - accuracy: 0.1150\n",
      "Epoch 7/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 3.1110 - accuracy: 0.1150\n",
      "Epoch 8/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 3.0321 - accuracy: 0.1150\n",
      "Epoch 9/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 2.9403 - accuracy: 0.2124\n",
      "Epoch 10/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 2.9518 - accuracy: 0.1681\n",
      "Epoch 11/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 2.7599 - accuracy: 0.2389\n",
      "Epoch 12/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 2.6928 - accuracy: 0.2743\n",
      "Epoch 13/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 2.4734 - accuracy: 0.3097\n",
      "Epoch 14/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 2.3408 - accuracy: 0.3274\n",
      "Epoch 15/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 2.3182 - accuracy: 0.3363\n",
      "Epoch 16/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 2.1462 - accuracy: 0.4159\n",
      "Epoch 17/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 2.0188 - accuracy: 0.4690\n",
      "Epoch 18/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 2.0746 - accuracy: 0.3717\n",
      "Epoch 19/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.9032 - accuracy: 0.4867\n",
      "Epoch 20/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.7460 - accuracy: 0.5221\n",
      "Epoch 21/500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 1.7281 - accuracy: 0.4513\n",
      "Epoch 22/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.6949 - accuracy: 0.5398\n",
      "Epoch 23/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.6593 - accuracy: 0.5664\n",
      "Epoch 24/500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 1.4846 - accuracy: 0.6460\n",
      "Epoch 25/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.4870 - accuracy: 0.6018\n",
      "Epoch 26/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.3429 - accuracy: 0.6195\n",
      "Epoch 27/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.3368 - accuracy: 0.6283\n",
      "Epoch 28/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 1.2527 - accuracy: 0.6372\n",
      "Epoch 29/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.0845 - accuracy: 0.7080\n",
      "Epoch 30/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.2741 - accuracy: 0.6372\n",
      "Epoch 31/500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 1.0101 - accuracy: 0.7788\n",
      "Epoch 32/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8868 - accuracy: 0.7434\n",
      "Epoch 33/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.0659 - accuracy: 0.6903\n",
      "Epoch 34/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.9169 - accuracy: 0.7257\n",
      "Epoch 35/500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.8980 - accuracy: 0.7168\n",
      "Epoch 36/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7757 - accuracy: 0.7965\n",
      "Epoch 37/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.9119 - accuracy: 0.7080\n",
      "Epoch 38/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6823 - accuracy: 0.8319\n",
      "Epoch 39/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8261 - accuracy: 0.7522\n",
      "Epoch 40/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8922 - accuracy: 0.7080\n",
      "Epoch 41/500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.7130 - accuracy: 0.7965\n",
      "Epoch 42/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6057 - accuracy: 0.8319\n",
      "Epoch 43/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8104 - accuracy: 0.7434\n",
      "Epoch 44/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6335 - accuracy: 0.7965\n",
      "Epoch 45/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5413 - accuracy: 0.8496\n",
      "Epoch 46/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4948 - accuracy: 0.8850\n",
      "Epoch 47/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5911 - accuracy: 0.8230\n",
      "Epoch 48/500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.5716 - accuracy: 0.8407\n",
      "Epoch 49/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5670 - accuracy: 0.8496\n",
      "Epoch 50/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.8584\n",
      "Epoch 51/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.8938\n",
      "Epoch 52/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5852 - accuracy: 0.8230\n",
      "Epoch 53/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4892 - accuracy: 0.8761\n",
      "Epoch 54/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.5647 - accuracy: 0.8584\n",
      "Epoch 55/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4851 - accuracy: 0.8673\n",
      "Epoch 56/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5270 - accuracy: 0.8319\n",
      "Epoch 57/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4115 - accuracy: 0.9204\n",
      "Epoch 58/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4582 - accuracy: 0.9204\n",
      "Epoch 59/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.9027\n",
      "Epoch 60/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.8584\n",
      "Epoch 61/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4765 - accuracy: 0.8673\n",
      "Epoch 62/500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.4563 - accuracy: 0.8938\n",
      "Epoch 63/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4068 - accuracy: 0.8761\n",
      "Epoch 64/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.9204\n",
      "Epoch 65/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3625 - accuracy: 0.8761\n",
      "Epoch 66/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2645 - accuracy: 0.9204\n",
      "Epoch 67/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3206 - accuracy: 0.9204\n",
      "Epoch 68/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.9027\n",
      "Epoch 69/500\n",
      "23/23 [==============================] - 0s 921us/step - loss: 0.3030 - accuracy: 0.8850\n",
      "Epoch 70/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8761\n",
      "Epoch 71/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3533 - accuracy: 0.8938\n",
      "Epoch 72/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.9115\n",
      "Epoch 73/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3177 - accuracy: 0.9204\n",
      "Epoch 74/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3225 - accuracy: 0.9027\n",
      "Epoch 75/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3252 - accuracy: 0.9381\n",
      "Epoch 76/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3161 - accuracy: 0.8938\n",
      "Epoch 77/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2869 - accuracy: 0.9027\n",
      "Epoch 78/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2330 - accuracy: 0.9292\n",
      "Epoch 79/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3354 - accuracy: 0.8938\n",
      "Epoch 80/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9558\n",
      "Epoch 81/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.9027\n",
      "Epoch 82/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.9381\n",
      "Epoch 83/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2967 - accuracy: 0.9115\n",
      "Epoch 84/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.9115\n",
      "Epoch 85/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.9204\n",
      "Epoch 86/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2252 - accuracy: 0.9115\n",
      "Epoch 87/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2772 - accuracy: 0.9115\n",
      "Epoch 88/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2687 - accuracy: 0.9469\n",
      "Epoch 89/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.9027\n",
      "Epoch 90/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2494 - accuracy: 0.9381\n",
      "Epoch 91/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2554 - accuracy: 0.9204\n",
      "Epoch 92/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2853 - accuracy: 0.9204\n",
      "Epoch 93/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.9381\n",
      "Epoch 94/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2286 - accuracy: 0.9204\n",
      "Epoch 95/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.9115\n",
      "Epoch 96/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3052 - accuracy: 0.8850\n",
      "Epoch 97/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2745 - accuracy: 0.9292\n",
      "Epoch 98/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2551 - accuracy: 0.9115\n",
      "Epoch 99/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8850\n",
      "Epoch 100/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.9646\n",
      "Epoch 101/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3147 - accuracy: 0.9292\n",
      "Epoch 102/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.8761\n",
      "Epoch 103/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8761\n",
      "Epoch 104/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9469\n",
      "Epoch 105/500\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.2846 - accuracy: 0.9115\n",
      "Epoch 106/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2254 - accuracy: 0.9381\n",
      "Epoch 107/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9558\n",
      "Epoch 108/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9469\n",
      "Epoch 109/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9735\n",
      "Epoch 110/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2550 - accuracy: 0.9115\n",
      "Epoch 111/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1929 - accuracy: 0.9292\n",
      "Epoch 112/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9558\n",
      "Epoch 113/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1751 - accuracy: 0.9292\n",
      "Epoch 114/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.9204\n",
      "Epoch 115/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2105 - accuracy: 0.9558\n",
      "Epoch 116/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2624 - accuracy: 0.9381\n",
      "Epoch 117/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.9469\n",
      "Epoch 118/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1952 - accuracy: 0.9292\n",
      "Epoch 119/500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.1714 - accuracy: 0.9558\n",
      "Epoch 120/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.9558\n",
      "Epoch 121/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2026 - accuracy: 0.9381\n",
      "Epoch 122/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9823\n",
      "Epoch 123/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9646\n",
      "Epoch 124/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9735\n",
      "Epoch 125/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9292\n",
      "Epoch 126/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9823\n",
      "Epoch 127/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1617 - accuracy: 0.9646\n",
      "Epoch 128/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2228 - accuracy: 0.9292\n",
      "Epoch 129/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.9381\n",
      "Epoch 130/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2134 - accuracy: 0.9115\n",
      "Epoch 131/500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.0983 - accuracy: 0.9823\n",
      "Epoch 132/500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.1590 - accuracy: 0.9381\n",
      "Epoch 133/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.9823\n",
      "Epoch 134/500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.1669 - accuracy: 0.9292\n",
      "Epoch 135/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2449 - accuracy: 0.9204\n",
      "Epoch 136/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9646\n",
      "Epoch 137/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.9646\n",
      "Epoch 138/500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.1523 - accuracy: 0.9469\n",
      "Epoch 139/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9558\n",
      "Epoch 140/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9646\n",
      "Epoch 141/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2349 - accuracy: 0.9204\n",
      "Epoch 142/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9646\n",
      "Epoch 143/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9292\n",
      "Epoch 144/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1746 - accuracy: 0.9381\n",
      "Epoch 145/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2096 - accuracy: 0.8850\n",
      "Epoch 146/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1320 - accuracy: 0.9735\n",
      "Epoch 147/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9469\n",
      "Epoch 148/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9469\n",
      "Epoch 149/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.9292\n",
      "Epoch 150/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9381\n",
      "Epoch 151/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.9469\n",
      "Epoch 152/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1364 - accuracy: 0.9558\n",
      "Epoch 153/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9558\n",
      "Epoch 154/500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.2179 - accuracy: 0.9469\n",
      "Epoch 155/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9735\n",
      "Epoch 156/500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.2226 - accuracy: 0.9292\n",
      "Epoch 157/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9558\n",
      "Epoch 158/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9381\n",
      "Epoch 159/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9735\n",
      "Epoch 160/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1587 - accuracy: 0.9558\n",
      "Epoch 161/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1329 - accuracy: 0.9646\n",
      "Epoch 162/500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.0925 - accuracy: 0.9646\n",
      "Epoch 163/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9204\n",
      "Epoch 164/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9469\n",
      "Epoch 165/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9646\n",
      "Epoch 166/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9735\n",
      "Epoch 167/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1603 - accuracy: 0.9381\n",
      "Epoch 168/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1212 - accuracy: 0.9646\n",
      "Epoch 169/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9735\n",
      "Epoch 170/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9735\n",
      "Epoch 171/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1658 - accuracy: 0.9381\n",
      "Epoch 172/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1517 - accuracy: 0.9381\n",
      "Epoch 173/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.9558\n",
      "Epoch 174/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1654 - accuracy: 0.9469\n",
      "Epoch 175/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9469\n",
      "Epoch 176/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2588 - accuracy: 0.8850\n",
      "Epoch 177/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9646\n",
      "Epoch 178/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.9381\n",
      "Epoch 179/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2729 - accuracy: 0.9027\n",
      "Epoch 180/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9469\n",
      "Epoch 181/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1356 - accuracy: 0.9646\n",
      "Epoch 182/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1622 - accuracy: 0.9292\n",
      "Epoch 183/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1006 - accuracy: 0.9646\n",
      "Epoch 184/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9823\n",
      "Epoch 185/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1213 - accuracy: 0.9735\n",
      "Epoch 186/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2229 - accuracy: 0.9204\n",
      "Epoch 187/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9646\n",
      "Epoch 188/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9558\n",
      "Epoch 189/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9558\n",
      "Epoch 190/500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.1623 - accuracy: 0.9558\n",
      "Epoch 191/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9469\n",
      "Epoch 192/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9646\n",
      "Epoch 193/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9735\n",
      "Epoch 194/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 0.9912\n",
      "Epoch 195/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1059 - accuracy: 0.9558\n",
      "Epoch 196/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9912\n",
      "Epoch 197/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9735\n",
      "Epoch 198/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9735\n",
      "Epoch 199/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9912\n",
      "Epoch 200/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9381\n",
      "Epoch 201/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.9469\n",
      "Epoch 202/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9646\n",
      "Epoch 203/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9646\n",
      "Epoch 204/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.9646\n",
      "Epoch 205/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9558\n",
      "Epoch 206/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9381\n",
      "Epoch 207/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9469\n",
      "Epoch 208/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9558\n",
      "Epoch 209/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9646\n",
      "Epoch 210/500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.1951 - accuracy: 0.9292\n",
      "Epoch 211/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9469\n",
      "Epoch 212/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9381\n",
      "Epoch 213/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9558\n",
      "Epoch 214/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9646\n",
      "Epoch 215/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9823\n",
      "Epoch 216/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9823\n",
      "Epoch 217/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9469\n",
      "Epoch 218/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9558\n",
      "Epoch 219/500\n",
      "23/23 [==============================] - 0s 884us/step - loss: 0.1928 - accuracy: 0.9292\n",
      "Epoch 220/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.9735\n",
      "Epoch 221/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9381\n",
      "Epoch 222/500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.1491 - accuracy: 0.9646\n",
      "Epoch 223/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9558\n",
      "Epoch 224/500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.1148 - accuracy: 0.9735\n",
      "Epoch 225/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9646\n",
      "Epoch 226/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.9823\n",
      "Epoch 227/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1656 - accuracy: 0.9558\n",
      "Epoch 228/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9469\n",
      "Epoch 229/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.9823\n",
      "Epoch 230/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9558\n",
      "Epoch 231/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9735\n",
      "Epoch 232/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.9558\n",
      "Epoch 233/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9381\n",
      "Epoch 234/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9823\n",
      "Epoch 235/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9735\n",
      "Epoch 236/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9292\n",
      "Epoch 237/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9735\n",
      "Epoch 238/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1648 - accuracy: 0.9558\n",
      "Epoch 239/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9823\n",
      "Epoch 240/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9646\n",
      "Epoch 241/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9735\n",
      "Epoch 242/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9646\n",
      "Epoch 243/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9646\n",
      "Epoch 244/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9823\n",
      "Epoch 245/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9646\n",
      "Epoch 246/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9646\n",
      "Epoch 247/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9823\n",
      "Epoch 248/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0480 - accuracy: 0.9823\n",
      "Epoch 249/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.9735\n",
      "Epoch 250/500\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.0864 - accuracy: 0.9823\n",
      "Epoch 251/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9823\n",
      "Epoch 252/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9823\n",
      "Epoch 253/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2513 - accuracy: 0.9469\n",
      "Epoch 254/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9735\n",
      "Epoch 255/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9646\n",
      "Epoch 256/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9823\n",
      "Epoch 257/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9646\n",
      "Epoch 258/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9735\n",
      "Epoch 259/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9469\n",
      "Epoch 260/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2218 - accuracy: 0.9381\n",
      "Epoch 261/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9735\n",
      "Epoch 262/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0983 - accuracy: 0.9646\n",
      "Epoch 263/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9558\n",
      "Epoch 264/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9558\n",
      "Epoch 265/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9735\n",
      "Epoch 266/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.9823\n",
      "Epoch 267/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9469\n",
      "Epoch 268/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0586 - accuracy: 0.9735\n",
      "Epoch 269/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9735\n",
      "Epoch 270/500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.0550 - accuracy: 0.9823\n",
      "Epoch 271/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9469\n",
      "Epoch 272/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9646\n",
      "Epoch 273/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9558\n",
      "Epoch 274/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9646\n",
      "Epoch 275/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.9912\n",
      "Epoch 276/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1921 - accuracy: 0.9292\n",
      "Epoch 277/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9558\n",
      "Epoch 278/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.9735\n",
      "Epoch 279/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9558\n",
      "Epoch 280/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1150 - accuracy: 0.9558\n",
      "Epoch 281/500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.0864 - accuracy: 0.9735\n",
      "Epoch 282/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9558\n",
      "Epoch 283/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9558\n",
      "Epoch 284/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.9912\n",
      "Epoch 285/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9735\n",
      "Epoch 286/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9381\n",
      "Epoch 287/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1016 - accuracy: 0.9735\n",
      "Epoch 288/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0461 - accuracy: 0.9912\n",
      "Epoch 289/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1028 - accuracy: 0.9469\n",
      "Epoch 290/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1184 - accuracy: 0.9558\n",
      "Epoch 291/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9558\n",
      "Epoch 292/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9823\n",
      "Epoch 293/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0984 - accuracy: 0.9735\n",
      "Epoch 294/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1252 - accuracy: 0.9558\n",
      "Epoch 295/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.9823\n",
      "Epoch 296/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9558\n",
      "Epoch 297/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9735\n",
      "Epoch 298/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0897 - accuracy: 0.9735\n",
      "Epoch 299/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0969 - accuracy: 0.9558\n",
      "Epoch 300/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9558\n",
      "Epoch 301/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1023 - accuracy: 0.9558\n",
      "Epoch 302/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9469\n",
      "Epoch 303/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0777 - accuracy: 0.9735\n",
      "Epoch 304/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1306 - accuracy: 0.9823\n",
      "Epoch 305/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.9912\n",
      "Epoch 306/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.9912\n",
      "Epoch 307/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0445 - accuracy: 0.9912\n",
      "Epoch 308/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.9823\n",
      "Epoch 309/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9823\n",
      "Epoch 310/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0358 - accuracy: 0.9912\n",
      "Epoch 311/500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.0584 - accuracy: 0.9912\n",
      "Epoch 312/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9735\n",
      "Epoch 313/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9558\n",
      "Epoch 314/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9292\n",
      "Epoch 315/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0780 - accuracy: 0.9735\n",
      "Epoch 316/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9469\n",
      "Epoch 317/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9823\n",
      "Epoch 318/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0497 - accuracy: 0.9912\n",
      "Epoch 319/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9735\n",
      "Epoch 320/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9735\n",
      "Epoch 321/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.0327 - accuracy: 1.0000\n",
      "Epoch 322/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9558\n",
      "Epoch 323/500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.0967 - accuracy: 0.9558\n",
      "Epoch 324/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.9735\n",
      "Epoch 325/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9735\n",
      "Epoch 326/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9735\n",
      "Epoch 327/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0866 - accuracy: 0.9646\n",
      "Epoch 328/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1707 - accuracy: 0.9558\n",
      "Epoch 329/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0723 - accuracy: 0.9735\n",
      "Epoch 330/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1659 - accuracy: 0.9558\n",
      "Epoch 331/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0868 - accuracy: 0.9735\n",
      "Epoch 332/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.9735\n",
      "Epoch 333/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9558\n",
      "Epoch 334/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1240 - accuracy: 0.9735\n",
      "Epoch 335/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9735\n",
      "Epoch 336/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 1.0000\n",
      "Epoch 337/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9823\n",
      "Epoch 338/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9646\n",
      "Epoch 339/500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.0750 - accuracy: 0.9735\n",
      "Epoch 340/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9646\n",
      "Epoch 341/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0705 - accuracy: 0.9735\n",
      "Epoch 342/500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.0520 - accuracy: 0.9823\n",
      "Epoch 343/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9735\n",
      "Epoch 344/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9558\n",
      "Epoch 345/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.9912\n",
      "Epoch 346/500\n",
      "23/23 [==============================] - 0s 906us/step - loss: 0.0646 - accuracy: 0.9646\n",
      "Epoch 347/500\n",
      "23/23 [==============================] - 0s 859us/step - loss: 0.0772 - accuracy: 0.9735\n",
      "Epoch 348/500\n",
      "23/23 [==============================] - 0s 959us/step - loss: 0.0568 - accuracy: 0.9823\n",
      "Epoch 349/500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.0669 - accuracy: 0.9735\n",
      "Epoch 350/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9646\n",
      "Epoch 351/500\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.0949 - accuracy: 0.9735\n",
      "Epoch 352/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9735\n",
      "Epoch 353/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9558\n",
      "Epoch 354/500\n",
      "23/23 [==============================] - 0s 908us/step - loss: 0.0886 - accuracy: 0.9646\n",
      "Epoch 355/500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.0341 - accuracy: 0.9912\n",
      "Epoch 356/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9735\n",
      "Epoch 357/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0627 - accuracy: 0.9735\n",
      "Epoch 358/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9558\n",
      "Epoch 359/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9381\n",
      "Epoch 360/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9646\n",
      "Epoch 361/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1032 - accuracy: 0.9735\n",
      "Epoch 362/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1204 - accuracy: 0.9381\n",
      "Epoch 363/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0627 - accuracy: 0.9912\n",
      "Epoch 364/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9646\n",
      "Epoch 365/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9469\n",
      "Epoch 366/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9558\n",
      "Epoch 367/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9558\n",
      "Epoch 368/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9558\n",
      "Epoch 369/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0847 - accuracy: 0.9558\n",
      "Epoch 370/500\n",
      "23/23 [==============================] - 0s 903us/step - loss: 0.0398 - accuracy: 0.9823\n",
      "Epoch 371/500\n",
      "23/23 [==============================] - 0s 868us/step - loss: 0.1096 - accuracy: 0.9735\n",
      "Epoch 372/500\n",
      "23/23 [==============================] - 0s 904us/step - loss: 0.0705 - accuracy: 0.9646\n",
      "Epoch 373/500\n",
      "23/23 [==============================] - 0s 908us/step - loss: 0.0712 - accuracy: 0.9735\n",
      "Epoch 374/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0518 - accuracy: 0.9912\n",
      "Epoch 375/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9912\n",
      "Epoch 376/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0470 - accuracy: 0.9735\n",
      "Epoch 377/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 378/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9469\n",
      "Epoch 379/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0777 - accuracy: 0.9558\n",
      "Epoch 380/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 0.9912\n",
      "Epoch 381/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1281 - accuracy: 0.9469\n",
      "Epoch 382/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0843 - accuracy: 0.9735\n",
      "Epoch 383/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.9558\n",
      "Epoch 384/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9469\n",
      "Epoch 385/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1617 - accuracy: 0.9381\n",
      "Epoch 386/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1057 - accuracy: 0.9469\n",
      "Epoch 387/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9469\n",
      "Epoch 388/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0519 - accuracy: 0.9823\n",
      "Epoch 389/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1121 - accuracy: 0.9646\n",
      "Epoch 390/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0471 - accuracy: 0.9823\n",
      "Epoch 391/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9381\n",
      "Epoch 392/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9735\n",
      "Epoch 393/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0601 - accuracy: 0.9823\n",
      "Epoch 394/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.9823\n",
      "Epoch 395/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0421 - accuracy: 0.9912\n",
      "Epoch 396/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0459 - accuracy: 0.9823\n",
      "Epoch 397/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9558\n",
      "Epoch 398/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.9646\n",
      "Epoch 399/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9735\n",
      "Epoch 400/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0336 - accuracy: 1.0000\n",
      "Epoch 401/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9735\n",
      "Epoch 402/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9735\n",
      "Epoch 403/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9469\n",
      "Epoch 404/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9735\n",
      "Epoch 405/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1261 - accuracy: 0.9469\n",
      "Epoch 406/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.9823\n",
      "Epoch 407/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9735\n",
      "Epoch 408/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0904 - accuracy: 0.9646\n",
      "Epoch 409/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0641 - accuracy: 0.9912\n",
      "Epoch 410/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0411 - accuracy: 0.9912\n",
      "Epoch 411/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0466 - accuracy: 0.9912\n",
      "Epoch 412/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0467 - accuracy: 0.9735\n",
      "Epoch 413/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.9735\n",
      "Epoch 414/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0719 - accuracy: 0.9735\n",
      "Epoch 415/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9558\n",
      "Epoch 416/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0382 - accuracy: 0.9823\n",
      "Epoch 417/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.9646\n",
      "Epoch 418/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0382 - accuracy: 0.9912\n",
      "Epoch 419/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0470 - accuracy: 0.9823\n",
      "Epoch 420/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0937 - accuracy: 0.9469\n",
      "Epoch 421/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9735\n",
      "Epoch 422/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0284 - accuracy: 0.9912\n",
      "Epoch 423/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9735\n",
      "Epoch 424/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 425/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0467 - accuracy: 0.9823\n",
      "Epoch 426/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0473 - accuracy: 0.9823\n",
      "Epoch 427/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.9646\n",
      "Epoch 428/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0442 - accuracy: 0.9823\n",
      "Epoch 429/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.9735\n",
      "Epoch 430/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9735\n",
      "Epoch 431/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9646\n",
      "Epoch 432/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0944 - accuracy: 0.9646\n",
      "Epoch 433/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9735\n",
      "Epoch 434/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9735\n",
      "Epoch 435/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9646\n",
      "Epoch 436/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.0807 - accuracy: 0.9646\n",
      "Epoch 437/500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.0479 - accuracy: 0.9823\n",
      "Epoch 438/500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.0543 - accuracy: 0.9823\n",
      "Epoch 439/500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.0814 - accuracy: 0.9823\n",
      "Epoch 440/500\n",
      "23/23 [==============================] - 0s 915us/step - loss: 0.0594 - accuracy: 0.9823\n",
      "Epoch 441/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9646\n",
      "Epoch 442/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9823\n",
      "Epoch 443/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0712 - accuracy: 0.9735\n",
      "Epoch 444/500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.0848 - accuracy: 0.9469\n",
      "Epoch 445/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9469\n",
      "Epoch 446/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9558\n",
      "Epoch 447/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9646\n",
      "Epoch 448/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9912\n",
      "Epoch 449/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9469\n",
      "Epoch 450/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9823\n",
      "Epoch 451/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9646\n",
      "Epoch 452/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0481 - accuracy: 0.9823\n",
      "Epoch 453/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0263 - accuracy: 0.9912\n",
      "Epoch 454/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9823\n",
      "Epoch 455/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9469\n",
      "Epoch 456/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.9735\n",
      "Epoch 457/500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0432 - accuracy: 0.9912\n",
      "Epoch 458/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1074 - accuracy: 0.9735\n",
      "Epoch 459/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.9735\n",
      "Epoch 460/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0479 - accuracy: 0.9646\n",
      "Epoch 461/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0289 - accuracy: 0.9912\n",
      "Epoch 462/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9646\n",
      "Epoch 463/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0567 - accuracy: 0.9735\n",
      "Epoch 464/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9735\n",
      "Epoch 465/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0693 - accuracy: 0.9735\n",
      "Epoch 466/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.9735\n",
      "Epoch 467/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9558\n",
      "Epoch 468/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0909 - accuracy: 0.9735\n",
      "Epoch 469/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0527 - accuracy: 0.9823\n",
      "Epoch 470/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0891 - accuracy: 0.9469\n",
      "Epoch 471/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0562 - accuracy: 0.9735\n",
      "Epoch 472/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9735\n",
      "Epoch 473/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 474/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0997 - accuracy: 0.9558\n",
      "Epoch 475/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0357 - accuracy: 0.9823\n",
      "Epoch 476/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0476 - accuracy: 0.9823\n",
      "Epoch 477/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0802 - accuracy: 0.9735\n",
      "Epoch 478/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0673 - accuracy: 0.9735\n",
      "Epoch 479/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9646\n",
      "Epoch 480/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.9823\n",
      "Epoch 481/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0478 - accuracy: 0.9823\n",
      "Epoch 482/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0342 - accuracy: 0.9823\n",
      "Epoch 483/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0473 - accuracy: 0.9912\n",
      "Epoch 484/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0556 - accuracy: 0.9912\n",
      "Epoch 485/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9646\n",
      "Epoch 486/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9469\n",
      "Epoch 487/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 0.9735\n",
      "Epoch 488/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9735\n",
      "Epoch 489/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0479 - accuracy: 0.9912\n",
      "Epoch 490/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0516 - accuracy: 0.9823\n",
      "Epoch 491/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0460 - accuracy: 0.9912\n",
      "Epoch 492/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0449 - accuracy: 0.9823\n",
      "Epoch 493/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.9735\n",
      "Epoch 494/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9735\n",
      "Epoch 495/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9558\n",
      "Epoch 496/500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9558\n",
      "Epoch 497/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9735\n",
      "Epoch 498/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9735\n",
      "Epoch 499/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0348 - accuracy: 0.9912\n",
      "Epoch 500/500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0536 - accuracy: 0.9823\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_x, train_y, epochs = 500, batch_size = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Finishing the Chatbot\n",
    "\n",
    "Now that we have everything we need, we can finish the chatbot!\n",
    "\n",
    "As a reminder, these are the methods you need to complete:\n",
    "1. `predict_tag()`\n",
    "2. `get_response()`\n",
    "3. `chat()`\n",
    "\n",
    "Again, this is quite a short notebook, so feel free to spend some time looking over and finishing the first part if you need to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Predicting the Tag\n",
    "\n",
    "Last week we created a neural network that took our training data as an input, and matched it to a tag output. Now, we need to predict tags using custom user inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `predict_tag()`, which does the following\n",
    "1. Sets `process_input` to the cleaned and tokenized `user_input`\n",
    "2. Sets `bag_input` to the bag representation of `process_input`\n",
    "3. Calculates `pred_tag_values` by using our `model` and the `predict()` function\n",
    "4. Finds the values for `max_value_tag` and `probability`, the maximum index and value, respectively, of `pred_tag_values`\n",
    "5. Gets the value of `pred_tag` using `max_value_tag` and the list of `tags`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the <code>process_words()</code> helper function with <code>user_input</code> as the parameter</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the <code>build_bag()</code> helper function with <code>all_words</code> and <code>process_input</code> as the parameters</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>model.predict()</code> will get the values you need</li>\n",
    "    <li>The argument for <code>model.predict()</code> should be <code>bag_input</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>np.argmax()</code> will return the index of the maximum value of a numpy array. <a href = https://numpy.org/doc/stable/reference/generated/numpy.argmax.html>Documentation</a></li>\n",
    "    <li><code>np.max()</code> will return the maximum value of a numpy array. <a href = https://numpy.org/doc/stable/reference/generated/numpy.amax.html>Documentation</a></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 5</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>pred_tag</code> will be the value of the numpy array <code>tags</code> at index <code>max_value_tag</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tag(user_input, model):\n",
    "    # [your code here] - tokenize/clean inputs\n",
    "    process_input = process_words(user_input)\n",
    "    \n",
    "    # [your code here] - build the bag\n",
    "    bag_input = build_bag(all_words, process_input)\n",
    "    bag_input = np.array([bag_input]) # note: convert to a numpy array\n",
    "    \n",
    "    # [your code here] - get our predicted values\n",
    "    pred_tag_values = model.predict(bag_input)\n",
    "    pred_tag_values = pred_tag_values[0] # note: flatten the 2-d array\n",
    "    \n",
    "    # [your code here] - get the index and value of the largest probability value\n",
    "    max_value_tag = np.argmax(pred_tag_values)\n",
    "    probability = np.max(pred_tag_values)\n",
    "    \n",
    "    # [your code here] - predict the tag and return\n",
    "    pred_tag = tags[max_value_tag]\n",
    "    return (pred_tag, probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('greeting', 0.50441056)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try your own inputs here and make sure that the tag makes sense!\n",
    "# look at the probability for the bot's confidence level\n",
    "custom_input = \"How are you today?\"\n",
    "predict_tag(custom_input, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Getting a Response\n",
    "\n",
    "Now that we have a way to predict our tags, we need to get a user input from this tag "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `get_response()`, which does the following\n",
    "1. Sets `pred_tag` and `probability` using our helper method `predict_tag()`\n",
    "2. Gets the correct responses from the `tag_responses` dictionary. If the `probability` is not high enough, it should equal `tag_responses[\"noanswer\"]`\n",
    "3. Sets the boolean value of `should_exit_bot`, which should be `true` if the predicted tag is `\"goodbye\"` or `\"thanks\"`\n",
    "4. Get a random `response` from our `responses`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use the <code>predict_tag()</code> helper function with <code>user_input</code> and <code>thetas</code> as the parameters</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Write your code in the format <code>responses = [_____] if [_____] else [_____]</code></li>\n",
    "    <li>The first blank should be the value from the dictionary, <code>tag_responses[pred_tag]</code></li>\n",
    "    <li>The second blank should be the condition when <code>probability > error_margin</code></li>\n",
    "    <li>The final blank should be the invalid response tag, <code>tag_responses[\"noanswer\"]</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>The value should be <code>true</code> with the goodbye tag, so set <code>should_exit_bot</code> equal to <code>pred_tag == \"goodbye\" or pred_tag == \"thanks\"</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li><code>np.random.choice()</code> will return a random element from an array. Read more about it <a href = https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html>here</a></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(user_input, model, error_margin):\n",
    "    # [your code here] - get the predicted tag and probability\n",
    "    pred_tag, probability = predict_tag(user_input, model)\n",
    "    \n",
    "    # [your code here] - get a list of different responses\n",
    "    responses = tag_responses[pred_tag] if probability > error_margin else tag_responses[\"noanswer\"]\n",
    "    \n",
    "    # [your code here] - check if we should exit the bot\n",
    "    should_exit_bot = pred_tag == 'goodbye' or pred_tag == \"thanks\"\n",
    "    \n",
    "    # [your code here] - get the response\n",
    "    response = np.random.choice(responses)\n",
    "    \n",
    "    # return the variables\n",
    "    return (response, should_exit_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Good to see you again', False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try your own inputs here and check the responses! (remember the second output should only be true for goodbye messages)\n",
    "custom_input = \"How are you today?\"\n",
    "get_response(custom_input, model, 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating the Chat User Interface\n",
    "\n",
    "Everything is set up! We just need to put it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the method `chat()`, which puts it all together:\n",
    "1. Sets `user_input` using Python's `input()` function. I recommend putting in `human_prefix` as the parameter. Read more about it [here](https://www.w3schools.com/python/ref_func_input.asp)\n",
    "2. Finds `response` and `should_exit` using our helper method `get_response`\n",
    "3. Prints the `response` preceded by the `robot_prefix`\n",
    "4. Sets `continue_chat` appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints for Step 1</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>If you set <code>user_input</code> equal to <code>input()</code>, it will automatically store the user's input</li>\n",
    "    <li>Set the parameter of <code>input()</code> to be <code>human_prefix</code> to improve your UI</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 2</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Call <code>get_response()</code> with <code>user_input</code>, <code>thetas</code> and <code>0.25</code> as the parameters. The last value can be anything, depending on how accurate you want your bot to be</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 3</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Use string concatenation to combine the strings</li>\n",
    "    <li>Print <code>robot_prefix + response</code> to the console</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hint for Step 4</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>We should continue when we shouldn't exit, so set <code>continue_chat = not should_exit</code></li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat():\n",
    "    # initialize variables\n",
    "    continue_chat = True\n",
    "    robot_prefix = \"Bot: \"\n",
    "    human_prefix = \"You: \"\n",
    "    \n",
    "    # give an introduction\n",
    "    print(robot_prefix + \"Hi! I am a bot offering support for Taco Bell. Ask me what I can do! To exit, say goodbye\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # continue while the user doesn't say goodbye\n",
    "    while (continue_chat):\n",
    "        # [your code here] - get the user input from the console\n",
    "        user_input = input(human_prefix)\n",
    "        \n",
    "        # [your code here] - get the response and exit condition from the helper function\n",
    "        response, should_exit = get_response(user_input, model, 0.25)\n",
    "        \n",
    "        # [your code here] - print the bot's response \n",
    "        print(robot_prefix +  response)\n",
    "        print(\"\")\n",
    "        \n",
    "        # [your code here] - set the exit condition\n",
    "        continue_chat = not should_exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's everything you need! If you made it this far, congratulations, you just made your first chat bot! Run the method below to test it out\n",
    "\n",
    "If you want to add more patterns/responses to the bot, you can modify `intents.json` to your liking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hi! I am a bot offering support for Taco Bell. Ask me what I can do! To exit, say goodbye\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Bot: Good to know!\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Bot: o what if I don't know what 'Armageddon' means? It's not the end of the world.\n",
      "\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Bot: All good..What about you?\n",
      "\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Bot: What has a thumb and four fingers but is not actually alive?.....Your Gloves!\n",
      "\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Bot: Yeah!\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chat()\n",
      "Cell \u001b[1;32mIn [32], line 14\u001b[0m, in \u001b[0;36mchat\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# continue while the user doesn't say goodbye\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39mwhile\u001b[39;00m (continue_chat):\n\u001b[0;32m     13\u001b[0m     \u001b[39m# [your code here] - get the user input from the console\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(human_prefix)\n\u001b[0;32m     16\u001b[0m     \u001b[39m# [your code here] - get the response and exit condition from the helper function\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     response, should_exit \u001b[39m=\u001b[39m get_response(user_input, model, \u001b[39m0.25\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py:1177\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allow_stdin:\n\u001b[0;32m   1174\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[0;32m   1175\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1176\u001b[0m     )\n\u001b[1;32m-> 1177\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[0;32m   1178\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[0;32m   1179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1180\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1181\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1182\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\ipykernel\\kernelbase.py:1219\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1218\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m-> 1219\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m   1220\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1221\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "722e8765311bc783d01f25953f3c9b79780613b39edb0509b118f4c4b0314537"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
